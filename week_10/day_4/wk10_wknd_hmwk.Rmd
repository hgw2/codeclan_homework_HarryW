---
title: "R Notebook"
output: html_notebook
---

```{r}
library(tidyverse)
library(janitor)
library(GGally)
library(modelr)
library(ggfortify)
library(lubridate)
library(broom)

```

```{r}
avocados <- read_csv("data/avocado.csv") %>% 
  clean_names()

glimpse(avocados)
```

```{r}
min(avocados$date)
max(avocados$date)


```
X1 is a row number so we will drop that 
Date - the date range is from Jan 2015 - March 2018, we could split this up into year and quarter. By spitting into quarter we create less dummy variables instead of month
There is already a year column so lets create a quarter, we also need to make sure that they are treadt as factors 


```{r}
clean_avocados <- avocados %>% 
  dplyr::select(-x1) %>% 
  mutate(quarter = factor(quarter(date)), .after = date) %>% 
  mutate(year = factor(year), .after = quarter) 
```

```{r}
avocados %>% 
  distinct(region) %>% 
  mutate(regions = case_when(
    str_detect(region, "Al|Bal|Bos|Buff|Harris|Hart|York|Engl|Phil|Pitts|Sy|North" ) ~ "north_east",
    str_detect(region, "At|Char|Dal|Hous|Jacks|Louis|Miami|Nash|Orlea|Orl|Pheo|Ral|Rich|Roa|Caro|Tamp|Mid|South") ~ "south",
    str_detect(region, "Boi|Ch|Cin|Col|Det|Grand|Great|India|St" ) ~ "mid_west",
    str_detect(region, "Cal|Den|Las|Los|Plain|Port|Sac|San|Spo|Sea|Phoe|Tex|West" ) ~ "west",
    TRUE ~ "group_region"
    
  )) %>% 
  filter(regions == "group_region")
```

Looking at the regions there appears to be both cities and regions, since we are tyring to predict the average price of the avocado I have groupped them into the smaller regions but I have added a flag column so I coan filter them in and out if they have an impact on the pirce.   

```{r}
clean_avocados <- clean_avocados %>% 
  mutate(grouped_market = region %in% c("California", "West", "SouthCentral", "GreatLakes",
                                        "Midsouth", "Southeast", "Northeast", "Plains",
                                        "TotalUS")) %>% 
   mutate(regions = case_when(
    str_detect(region, "Al|Bal|Bos|Buff|Harris|Hart|York|Engl|Phil|Pitts|Sy|North" ) ~ "north_east",
    str_detect(region, "At|Char|Dal|Hous|Jacks|Louis|Miami|Nash|Orlea|Orl|Pheo|Ral|Rich|Roa|Caro|Tamp|Mid|South") ~ "south",
    str_detect(region, "Boi|Ch|Cin|Col|Det|Grand|Great|India|St" ) ~ "mid_west",
    str_detect(region, "Cal|Den|Las|Los|Plain|Port|Sac|San|Spo|Sea|Phoe|Tex|West" ) ~ "west",
    TRUE ~ "us_wide"
    
  ), .before = grouped_market) %>%  
  dplyr::select(-c(region, date)) %>% 
  relocate(year, .after = quarter) %>% 
  mutate(regions = factor(regions),
         type = factor(type))
```



Final Check
```{r}
summary(clean_avocados)
```

There are No NA Values it seems that there are 338 Us wide values for average price and 3042 for regional markets. I will drop the US wide values and see what the results are. I will leave the grouped markets in their respective regions. if they have an affect on the model I will filter them out.

# Alias

```{r}


alias(lm(average_price ~ ., data = clean_avocados))
```

# Explanatory Model 



```{r}
clean_avocados %>% 
  ggplot()+
  aes(x = total_volume, y = average_price, colour = regions)+
  geom_point() 
  
```
It does appear that the US wide numbers are affecting the distribution so I will filter those out

```{r}
clean_avocados %>% 
  filter(regions != "us_wide") %>% 
  ggplot()+
  aes(x = total_volume, y = average_price, colour = regions)+
  geom_point() 
  
```
That provides a much better distribution 


```{r}
clean_avocados %>% 
  filter(regions != "us_wide") %>% 
  ggplot()+
  aes(x = total_volume, y = average_price, colour = grouped_market)+
  geom_point() 
  
```
```{r}
clean_avocados %>% 
  filter(grouped_market == F) %>% 
  ggplot()+
  aes(x = log(total_volume), y = average_price, colour = grouped_market)+
  geom_point() 
  
```
The shape fo the distribution is pretty much the same so I will keep them in 

A log transformation might be a good option



```{r}
clean_avocados %>% 
  ggplot(aes(x =average_price))+
  geom_histogram()
```

```{r}
clean_avocados <- clean_avocados %>% 
  filter(regions != "us_wide")
```


```{r}
library(HH)
ladder(average_price ~ total_volume, data = clean_avocados)
```

transformation doesn't offer a huge amount

```{r}
test <- clean_avocados %>% 
  mutate(test_column = if_else( x4046 > 0, log(x4046),0))

summary(test)
```

```{r}
n_data <- nrow(clean_avocados)
test_index <- sample(1:n_data, size = n_data*0.1)

test  <- slice(clean_avocados, test_index)
train <- slice(clean_avocados, -test_index)
```


```{r message=FALSE, warning=FALSE}
numeric_avocados <- train %>% 
  dplyr::select(where(is.numeric))

non_numeric_avocados <- train %>% 
  dplyr::select(average_price, !is.numeric)

ggpairs(numeric_avocados) 
ggpairs(non_numeric_avocados) 
```

x4046 provides the best correlation although there seems to be an odd shape of distribution.
Group Markets don't seem to have an impact on thebut there does seem to be a good variation on type

```{r}
mod1a <- lm(average_price ~ type, data = train)

autoplot(mod1a) 

summary(mod1a)
```
```{r}
mod1b <- lm(average_price ~ total_volume, data = train)

autoplot(mod1b) 

summary(mod1b)
```

There is quite a lot spread in that data lets try a transformation

```{r}
mod1b2 <- lm(average_price ~ log(total_volume), data = train)

autoplot(mod1b2) 

summary(mod1b2)
```



```{r}
mod1c <- lm(average_price ~ regions, data = train)

autoplot(mod1c) 

summary(mod1c)
```

type provide a r^2 values so I will go forward with that model 


```{r}
residual_avocado <- train %>% 
  add_residuals(mod1a) %>% 
  dplyr::select(-c(average_price, type))  

res_data_numeric <- residual_avocado %>% 
  dplyr::select(is.numeric)

res_data_nonnumeric <- residual_avocado %>% 
  dplyr::select( resid, !is.numeric)
```

```{r message=FALSE, warning=FALSE}
ggpairs(res_data_numeric)
ggpairs(res_data_nonnumeric)
```
```{r}
mod2a <- lm(average_price ~ type + regions, data = train)

autoplot(mod2a) 

summary(mod2a)
```
```{r}
mod2b <- lm(average_price ~ type + year, data = train)

autoplot(mod2b) 

summary(mod2b)
```
```{r}
mod2c <- lm(average_price ~ type + total_volume, data = train)

autoplot(mod2c) 

summary(mod2c)
```
```{r}
mod2c2 <- lm(average_price ~ type + log(total_volume), data = train)

autoplot(mod2c2) 

summary(mod2c2)
```
 
mod2a <- lm(average_price ~ type + regions, data = train) is the best
```{r}
anova(mod2a, mod1a)
```


```{r}
residual_avocado <- train %>% 
  add_residuals(mod2a) %>% 
  dplyr::select(-c(average_price, type, regions))  

res_data_numeric <- residual_avocado %>% 
  dplyr::select(is.numeric)

res_data_nonnumeric <- residual_avocado %>% 
  dplyr::select( resid, !is.numeric)
```

```{r message=FALSE, warning=FALSE}
ggpairs(res_data_numeric)
ggpairs(res_data_nonnumeric)
```


```{r}
mod3a <- lm(average_price ~ type + regions + year, data = train) 
summary(mod3a)
```
```{r}
mod3b <- lm(average_price ~ type + regions + quarter, data = train) 
summary(mod3b)
```
```{r}
mod3b <- lm(average_price ~ type + regions + log(total_volume), data = train) 
autoplot(mod3b)
summary(mod3b)
```

mod3b <- lm(average_price ~ type + regions + quarter, data = train)  is the best


```{r}
anova(mod3b, mod2a)
```

```{r}
residual_avocado <- train %>% 
  add_residuals(mod3b) %>% 
  dplyr::select(-c(average_price, type, regions, quarter))  

res_data_numeric <- residual_avocado %>% 
  dplyr::select(is.numeric)

res_data_nonnumeric <- residual_avocado %>% 
  dplyr::select( resid, !is.numeric)
```

```{r message=FALSE, warning=FALSE}
ggpairs(res_data_numeric)
ggpairs(res_data_nonnumeric)
```

```{r}
mod4a <- lm(average_price ~ type + regions + quarter + year, data = train)  
summary(mod4a)
```

```{r}
mod4b <- lm(average_price ~ type + regions + quarter + log(total_volume), data = train)  
summary(mod4b)
```

```{r}
anova(mod4a, mod3b)
```


```{r}
mod5a <- lm(average_price ~ type + regions + quarter + year + log(total_volume), data = train)  
summary(mod5a)
```
We are getting very litle for each addition so we will stick with mod4a and look at interations

mod4a <- lm(average_price ~ type + regions + quarter + year, data = train)  

type:regions

```{r}
mod5a  <- lm(average_price ~ type + regions + quarter + year + type:regions, data = train)  

summary(mod5a)
```

type:quarter
```{r}
mod5b  <- lm(average_price ~ type + regions + quarter + year + type:quarter, data = train)  

summary(mod5b)
```

type:year
```{r}
mod5c  <- lm(average_price ~ type + regions + quarter + year + type:year, data = train)  

summary(mod5c)
```

reagions:quarter
```{r}
mod5d  <- lm(average_price ~ type + regions + quarter + year + regions:quarter, data = train)  

summary(mod5d)
```

regions:year
```{r}
mod5e  <- lm(average_price ~ type + regions + quarter + year + regions:year, data = train)  

summary(mod5e)
```
quarter:year
```{r}
mod5f  <- lm(average_price ~ type + regions + quarter + year + quarter:year, data = train)  

summary(mod5e)
```


mod5d  <- lm(average_price ~ type + regions + quarter + year + regions:quarter, data = train)  

```{r}
anova(mod4a, mod5d)
```

```{r}
glance(mod5d)
glance(mod4a)
```

mod5d offers a better model

```{r}
mod5d <- lm(average_price ~ type + regions + quarter + year + regions:quarter, data = train)  
```


```{r}
library(relaimpo)
calc.relimp(mod5d, type = "lmg", rela = TRUE)
```
The type of avocado makes up 73% of the r2. the R2 is still pretty low though


## Predictive model 

```{r}
n_data <- nrow(train)
test_index <- sample(1:n_data, size = n_data*0.1)

test  <- slice(train, test_index)
train <- slice(train, -test_index)
```


